---
title: "Aplicación de Procesamiento de Lenguaje Natural en las ciencias sociales. Detección automática de tópicos en letras de tango"
author: "Germán Rosati (CONICET - IDAES/UNSAM - PIMSA)"
#output: html_notebook
#output : word_document
#output: html_document
output: 
        pdf_document:
                keep_tex: true
                latex_engine: xelatex
csl: apa.csl
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev='pdf')
```

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(tm)
library(topicmodels)

topicmodels_json_ldavis <- function(fitted, dtm){
        
        svd_tsne <- function(x) tsne::tsne(svd(x)$u)
        
        # Find required quantities
        phi <- as.matrix(posterior(fitted)$terms)
        theta <- as.matrix(posterior(fitted)$topics)
        vocab <- colnames(phi)
        term_freq <- slam::col_sums(dtm)
        
        # Convert to json
        json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
                                       vocab = vocab,
                                       mds.method = svd_tsne,
                                       plot.opts = list(xlab="tsne", ylab=""),
                                       doc.length = as.vector(table(dtm$i)),
                                       term.frequency = term_freq)
        
        return(json_lda)
}

get_topic_per_doc<-function(fitted_lda){
        tmResult <- posterior(fitted_lda)
        theta <- tmResult$topics
        beta <- tmResult$terms
        topicNames <- apply(terms(fitted_lda, 5), 2, paste, collapse = " ")  # reset topicnames      
        
        colnames(theta) <- topicNames
        return(theta)
}

get_topic_terms <- function(lda, topic_n, n_words=50){
        df <- data.frame(topic=topic_n,
                     term=lda@terms,
                     prob=exp(lda@beta[topic_n,])) %>%
                arrange(desc(prob)) %>%
                top_n(n_words)
        
        return(df)
}

df <- read_csv('../data/data_limpia_UNSL.csv')
df <- df %>%
        mutate(decada = ano - ano %% 10) %>%
        filter(!(ritmo %in% c('aire de malambo' , 'aire de zamba', 'chacarera', 'chamame', 'chamarrita', 'cueca', 'huella', 'habanera','gato', 'guajira', 'ranchera', 'samba', 'tonada', 'zamba', 'vals peruano', 'vidalita', 'tonada salteña')))

corpus <- df$letra
corpus <- str_replace_all(df$letra, "[\\r\\n\\t]*", " ")
corpus <- str_replace_all(df$letra, "[:punct:]*", "")
corpus <- Corpus(VectorSource(corpus))

corpus <- tm_map(corpus, content_transformer(tolower))
stp <- c(stopwords(kind = "es"),'pa', 'pa’', 'tambien','mientras','tan','tal','así','asi', 'mas', 'más','aquel', 'áquel')
corpus <- tm_map(corpus, removeWords, stp)

corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_contractions=FALSE)
corpus <- tm_map(corpus, removeNumbers)

DTM <- DocumentTermMatrix(corpus, control = list(minWordLength = 1))

lda_list <- readRDS('../models/lda_fits_limpios.RDS')

#map(lda_list, terms, 10)[seq(2,22,2)]

lda <- lda_list[[8]]
```

 
## Introducción

Es una obviedad decirlo, pero el lenguaje es parte integrante de la sociedad. Independientemente de posiciones idealistas o materialistas al respecto, lo cierto es que buena parte de las interacciones a lo largo y a lo ancho de la estructura social, se encuentran mediadas por el lenguaje y tiene como producto una gran cantidad de "textos"", muchos de los cuales no son escritos, pero muchos otros sí.

Desde las ciencias sociales se ha hecho énfasis en esta idea, llegando a extremos teóricos y metodológicos y afirmaciones temerarias tales como que la sociedad ES un texto o que la cultura puede ser interpretada de la misma forma y con las mismas herramientas con que se aborda un texto literario. Uno de los referentes más relevantes de de la antropología interpretativa escribe en uno de sus textos más famosos:

> The culture of a people is an ensemble of texts, themselves ensembles, which the anthropologist strains to read over the shoulders of those to whom they properly belong. [@geertz]

Este enfoque ha ido acompañado de posiciones metodológicas más bien ligadas al análisis literario o filosófico: los métodos vinculados a la deconstrucción y a la hermenéutica constituyen algunos ejemplos. En la cita anterior resalta un requisito metodológico importante de este conjunto de enfoques: el objeto (la cultura) debe ser tratado por investigador como si
fuera un texto, lo cual pone en un lugar central a la operación de “interpretación”. Así, las herramientas y técnicas metodológicas de las que se suele echar mano en este tipo de enfoques se encuentran más vinculadas al análisis literario, al análisis del discurso y a métodos que buscan la comprensión del detalle y el contexto. Dichos enfoques han sido objeto de diversas críticas [@reynoso1]. No es objeto del presente trabajo evaluar dichas críticas y solo se remarcará un aspecto relevante a tener en cuenta: en términos generales, las posturas interpretativistas tienden a poner el eje en la capacidad subjetiva del investigador para realizar la interpretación. Esto hace que buena parte de las investigaciones realizadas bajo este tipo de marcos teóricos presenten (potencialmente) una relativa falta de sistematicidad metodológica y que el peligro de la imposibilidad de replicación de sus resultados esté siempre latente.

Las técnicas abordadas en este artículo pueden ser de utilidad a las ciencias sociales dado que permiten realizar una sistematización (y, eventualmente, lograr un cierto grado de automatización) de los diversos pasos de preprocesamiento de un texto y habilitan la aplicación de métodos cuantitativos de análisis para una amplia diversidad de tareas (clasificación de textos, detección de temas y tópicos, etc.).

Un segundo punto a considerar en este tipo de abordajes se vincula al problema de la escala. Existen algunas aproximaciones basadas en la tradición hermenéutica que buscan llegar a un grado más alto de sistematización de los procesos y etapas del análisis. El Qualitative Content Analysis [@fang], por ejemplo, busca sistematizar las diversas etapas y decisiones metodológicas en el proceso de análisis de datos no estructurados textuales. El problema que surge aquí es la escala: la transcripción y codificación manual de corpus textuales limita fuertemente el tamaño de los corpus a analizar.

Una vez más, las técnicas de _Text Mining_ y _Natural Language Processing_ abren la posibilidad de escalar el trabajo de forma eficiente. En lugar de leer cada uno de los textos de un corpus, tarea que rápidamente se vuelve imposible, las técnicas de minería de texto permiten analizar de forma automática corpus de escalas notablemente grandes.

El presente trabajo tiene como objetivo general discutir algunas aproximaciones metodológicas al análsis automático de textos y presentar algunas de las potencialidades que tienen en el trabajo cotidiano de las ciencias sociales a partir de su aplicacion a un caso de estudio concreto: el análisis de los temas en un corpus de 6.200 letras de tango. Particularmente, nos centraremos en la discusión de algunos aspectos del flujo de trabajo para el análisis de texto y en la aplicación y discusión de una técnica específica para resolución de un problema general del Procesamiento de Lenguaje Natural: el modelo _Latent Dirichlet Allocation_ (LDA) para la detección de tópicos en un corpus.


## 2. Antecedentes metodológicos en el análisis de letras de tango

El análisis de contenido en letras de tango no es un tema nuevo y existe una gran cantidad de literatura al respecto. Es importante tener en cuenta que el objetivo central del artículo es ilustrar la aplicación de un proceso de trabajo y de algunas técnicas de análisis vinculadas al campo del NLP. Se centra, entonces, en la dimensión metodológica del problema. Es por ello que los textos reseñados esta sección son ilustrativos en esa dimensión.

Un abordaje común es el rastreo de un tópico o problema particular a lo largo de un corpus textual. En el caso del análisis de letras de tango un tópico habitual se vincula a los estudios de género. Irene López [-@lopez_i] aborda cuáles son las representaciones que se hacen de las mujeres sobre un corpus de aproximadamente 10 letras de tango de diferentes épocas y autores. A su vez, Juan Gasparri [-@gasparri1] busca identificar las formas en que las masculinidades (particularmente, la del “guapo” en sus diversas construcciones) aparecen en un corpus de alrededor de 16 textos. También, teniendo como eje la problemática de género,
Carolina Marchese [-@marchese1] rastrea la temática amorosa y el sesgo sentimental en aproximadamente 30 tangos de fines del siglo XIX y principios del XX^[El tamaño de los corpus está referido a la cantidad de textos que citan o mencionan los diferentes autores en cada uno de los trabajos citados. Lógicamente, es razonable asumir que solamente se cita una fracción -desconocida- de los textos analizados.].

A su vez, otros textos tienen un enfoque más amplio y tratan de relevar una mayor cantidad de temas o problemas en las letras de tango. Así, Lucía Willenpart[-@willenpart] rastrea e identifica algunos temas comunes: el amor, el duelo amoroso, la mujer, la madre, el tango mismo, etc.

Por otro lado, Cantón [-@canton] se pregunta por los objetos y sujetos de los tangos cantados por Carlos Gardel. Este estudio tiene un carácter cuantitativo, por lo cual analiza un corpus significativamente más grande que los anteriores: alrededor de 100 tangos.

De esta forma, en términos metodológicos es posible identificar tres rasgos de los textos reseñados:

1. no aparecen criterios claros para la confección del corpus, con todos los potenciales segos y problemas que esto conlleva
2. el tamaño de los corpus es entre pequeño (10 tangos) y mediano (100 tangos)
3. con la excepción del texto de Cantón [@canton], los textos son abordados a partir del rastreo de un tópico o pregunta particular en profundidad, privilegiándose un análisis interpretativo del contenido.


## 3. Construcción del corpus y flujo de trabajo

A los efectos de ilustrar un caso de aplicación de este tipo de técnicas, se presentará un flujo de trabajo clásico aplicado a la detección de tópicos en un corpus textual de letras de tango. El corpus fue construido a partir de un proceso de web scraping^[El _scraping_-literalmente, “raspado” o “rascado” consiste en la descarga y formateo de la información disponible en sitios web, información que generalmente no se encuentra en condiciones de ser trabajada de forma estadística [@mitchell].] Se descargaron todas las letras de tango disponibles en el sitio [todo tango](www.todotango.com). 

Además de las letras se descargó información accesoria sobre el tango en cuestión. Para ello se confeccionaron dos web crawlers sencillos^[El código puede ser consultado en nuestro repositorio http://www.github/gefero.]. 

El corpus final consiste en alrededor de 5700 letras de tango, agrupadas en un dataset con la siguiente estructura:

**Tabla 1. Ejemplo de base de datos utilizada**

| Título   	| Ritmo  	| Año  	| Compositor  	| Autor       	| Letra                                        	|
|----------	|--------	|------	|-------------	|-------------	|----------------------------------------------	|
| A bailar 	| Tango  	| 1943 	| D. Federico 	| H. Expósito 	| a bailar a bailar que la orquesta se va...   	|
| ...      	| ...    	| ...  	| ...         	| ...         	| ...                                          	|
| Malena   	| Tango  	| 1941 	| L. Demare   	| H. Manzi    	| malena canta el tango como ninguna...        	|
| Zurdo    	| Tango  	| S/D  	| A. Pontier  	| F. Silva    	| era del tiempo lindo que siempre es antes... 	|


Si bien el resto de la información recolectada podría ser utilizada, en este trabajo y para poder ilustrar de forma más clara el trabajo sobre texto crudo, nos centraremos solamente en el campo que contiene las letras. Ese será el corpus final.

Se trata de un caso típico de datos no estructurados: las letras constituyen texto libre y no parece respetarse la estructura tripartita del dato. Las filas sí representan una unidad (los tangos) pero no tenemos atributos, o en todo caso, solamente tenemos un atributo, la letra. El primer paso, entonces, poder pasar de esta representación no estructurada a una estructurada. Esta representación tendrá como objeto reducir la complejidad del texto, dado que “el lenguage es complejo. Pero no toda su complejidad es necesaria para analizar un texto de forma efectiva” [@grimmer].

### 3.1 Modelo _BoW_ (_Bag of words_)

Para llegar a esa representación estructurada^[Vale destacar que el flujo de trabajo descripto a continuación es uno posible -y bastante común-, pero de ninguna manera el único, ni necesariamente el "mejor" en términos absoltos. El flujo y las operaciones contenidas en el mismo deberán ser revisadas para cada problema particular[@grimmer].], será necesario pensar en una estructura de datos acorde a las necesidades del análisis. En primer lugar, la unidad de análisis serán los tangos individuales, por lo cual, cada fila en la matriz final será un tango. A su vez, la representación a utilizar será la siguiente: cada columna consistirá en un término $t$ del vocabulario general $V$ del corpus $C$. Finalmente, cada celda estará constituida por el conteo crudo de ocurrencias de cada palabra (columna) en cada documento (fila). Esta representación es la que se denomina _Bag of Words_ o “bolsa de palabras” y se dispone en una Matriz de
Frecuencias de Términos ($TFM$, por sus siglas en inglés).

Por ejemplo, para dos de los tangos analizados esta matriz tomaría la siguiente forma:

**Tabla 2. Ejemplo de matriz de frecuencia de términos cruda**

| Letra                                                     	| agua 	| blanda 	| cartel 	| cruel 	| el 	| en 	| era 	| la 	| manda 	| mas 	| propaganda 	| que 	|
|-----------------------------------------------------------	|------	|--------	|--------	|-------	|----	|----	|-----	|----	|-------	|-----	|------------	|-----	|
| Cruel en el cartel la propaganda manda cruel en el cartel 	| 0    	| 0      	| 2      	| 2     	| 2  	| 2  	| 0   	| 1  	| 1     	| 0   	| 1          	| 0   	|
| Era más blanda que el agua que el agua blanda             	| 2    	| 2      	| 0      	| 0     	| 2  	| 0  	| 1   	| 0  	| 0     	| 1   	| 0          	| 2   	|


Puede verse que al construir esta matriz la información sobre el orden de las palabras se ha perdido. En efecto, el orden de las columnas es ahora arbitrario (en este caso, lexicográfico) y no respeta la estructura secuencial de las palabras en un texto. Esta es una simplificación importante del modelo $BoW$. Esta limitación puede subsanarse parcialmente generando una TFM de bi-gramas (pares de palabras), tri-gramas (tripletas de palabras) o n-gramas. El costo es un crecimiento exponencial en la dimensionalidad de la TFM

Debe tenerse en cuenta que esta matriz se construye sobre el vocabulario $V$ , o sea el total de términos únicos sobre el corpus $C$. Así, $V$ incluiría a priori signos de puntuación, diferentes conjugaciones de verbos, sustantivos en singular o plural, etc. Esto hace que el vocabulario “crudo” de $C$ tienda a ser demasiado grande. Es por ello que, en la etapa de preprocesamiento del texto se utilizan algunas técnicas para reducir la complejidad y la extensión de $V$.

Un primer paso simple es la eliminación de lo que suelen  denominarse stopwords, básicamente artículos, preposiciones, conectores, etc. La lógica detrás de esta eliminación es que estas palabras se encuentran en todos los documentos $d$ de $C$ por lo cual aportan poca información acerca de su contenido.

**Tabla 3. Ejemplo de matriz de frecuencia de términos sin stopwords**

| Letra                                                     	| agua 	| blanda 	| cartel 	| cruel 	| manda 	| mas 	| propaganda 	|
|-----------------------------------------------------------	|------	|--------	|--------	|-------	|-------	|-----	|------------	|
| Cruel en el cartel la propaganda manda cruel en el cartel 	| 0    	| 0      	| 2      	| 2     	| 1     	| 0   	| 1          	|
| Era más blanda que el agua que el agua blanda             	| 2    	| 2      	| 0      	| 0     	| 0     	| 1   	| 0          	|



A partir de la eliminación de las _stopwords_ se obtiene en la tabla 3 una representación más resumida de la información contenida en $C$. No obstante, ésta no es la única operación disponible para reducir la complejidad de $C$.


### 3.2 Normalización de términos: _stemming_ y lematización

El paso siguiente consiste en la reducción de la diversidad de los términos $t$ de $V$ manteniendo su sentido. Existen dos técnicas básicas para lograr este resultado.

1. _Stemming_: remueve las declinaciones de las palabras con el objetivo de reducir la dimensionalidad de $V$. Aquellas palabras que remiten a un mismo concepto básico son reducidos a la misma raíz. Por ejemplo, familias, familia y familiar son reducidas a familia.
2. Lematización: Tiene el mismo objetivo y lógica que el _stemming_. La diferencia es que usa diccionarios, el contexto de las palabras y su función sintáctica para determinar su raíz. Así, logra discernir que mejor y mejorable remiten a la misma raíz bueno.

La diferencia entre ambas suele ser el tiempo de cómputo. En términos generales, el _stemming_ tiende a ser más rápido, dado que solamente requiere de reglas para truncar las palabras. Existen diferentes algoritmos para varios idiomas (algoritmo Porter, algoritmo Snowball, etc.).


### 3.3 Normalización de conteos

El último paso supone normalizar los valores de las celdas de la TFM Al momento de filtrar los stopwords se buscaba poder eliminar aquellas palabras muy frecuentes en todos los textos. Es posible extender este razonamiento para el resto de los términos de $V$. Así, pueden identificarse dos dimensiones de la frecuencia de dichos términos:

1. Un término $t$ es importante si es muy frecuente en un documento $d$ de $C$
2. A su vez, $t$ es más informativo del contenido de un documento $d$ si está presente en pocos documentos y no en todos los documentos de $C$. 

Es decir, resulta importante analizar la frecuencia de $t$ tanto en el documento d como en el corpus total $C$. Existen dos métricas para lograr este objetivo. Para la primera dimensión se parte del conteo crudo de $t$ en $d$: $c(t, d)$, es decir, cada celda de la MFT “cruda”. Es posible definir, entonces, una métrica llamamda _Term Frequency_ ($TF$) es decir, el conteo crudo normalizado por la extensión del documento (el total de términos en el documento):

$$ TF(t,d) = \frac{c(t,d)}{\sum_{t \in d} c(t,d)}$$

En relación a la informatividad de un término a lo largo de C, podemos definir la siguiente métrica, llamada _Document Frequency_ ($DF$):

$$ DF(t) = \log \frac{df(t)}{|C|}$$

donde $df(t)$ es la cantidad de documentos en C que contienen a t; $|C|$ es el tamaño del corpus. 

De esta forma, $DF$ informa acerca de la proporción de documentos que contienen a $t$. Cuanto mayor es $DF(t)$ menos informativo es $t$. Es por ello que se usa la inversa de esta métrica:

$$ IDF(t) = \log \frac{|C|}{df(t)}$$
$IDF(t)$ entonces, es mayor, cuanto menor es la frecuencia de $t$ en $C$, es decir, cuanto más informativo es $t$.

Podemos combinar ambas dimensiones en una métrica resumen, llamada $TF−IDF$, _Term Frequency-Inverse Document Frequency_:

$$TF-IDF(t) = \frac{c(t,d)}{\sum_{t \in d} c(t,d)} \times \log \frac{|C|}{df(t)} = TF(t,d) \times IDF(t) $$

valores altos de $TF-IDF(t)$. O sea, términos $t$ frecuentes en $d$ y poco frecuentes en $C$.

No obstante, estas operaciones mencionadas, en ciertos casos (como el del corpus en cuestión) suele suceder que el conteo crudo $c(t,c)$ de términos fucione de forma aceptable. En el caso específico de este trabajo, luego de evaluar ambas alternativas se optó por utilizar $c(t, d)$ como métrica en la TFM^[Para una discusión al respecto de estas métricas, puede verse [@wind]]. Para resumir, entonces, el preprocesamiento realizado para este corpus:

1. normalización a minúsculas
2. eliminación de _stopwords_
3. elimnación de puntuación y 
4. eliminación de caracteres extraños y dígitos


## 4. Detección de tópicos

Sobre el corpus ya preprocesado es que se buscará detectar tópicos. Existen varias técnicas para la detección automática de tópicos en corpus textuales, algunas de las cuales están basadas en alguna forma de descomposición de la TFM^[Existen otros métodos para la detección de tópicos, basados más bien en la descomposición de la TFM en dos componentes, una matriz de documentos x tópicos y una matriz de términos x tópicos, tales como _Non Negative Matrix Factorization_ y _Latente Semantic Analysis_ [@hassani].]. Para el presente trabajo se utilizará una de las más conocidas: _Latent Dirichlet Allocation_ o LDA.

La intuición detrás de LDA [@blei1] es que cada $d$ de $C$ puede exhibir varios tópicos, es decir, puede hablar de varios temas simultáneamente. Por ejemplo, al analizar un tango como “Malena” de Homero Manzi, se observa que habla de diferentes temas: del amor, del tango, del barrio, etc. La idea detrás de LDA es poder operacionalizar esta intuición a través de un modelo generativo, es decir, asume la existencia de un “proceso generador de textos”.

Más formalmente^[Esta sección se basa en [@blei1]] un tópico se define como una distribución de probabilidad a lo largo de un vocabulario $V$ fijo. Por ejemplo, si existiera un tópico como sentimientos palabras como “amor”, “pena”, “sufrimiento”, deberían tener altas probabilidades para este tópico. En cambio, palabras como “ciudad”, “barrio” estarían más asociadas a un tópico acerca de la ciudad. Un supuesto fuerte de LDA es que los tópicos existen previamente a los documentos.

Ahora bien, para cada documento $d$ en el corpus $C$ se generan las palabras $w$ que lo componen en un proceso de dos etapas:

1. Se selecciona de forma aleatoria una distribución de tópicos para $d$
2. Para cada palabra ($w$) en $d$
   i) se selecciona aleatoriamente un tópico de la distribución general de tópicos 
   ii) se selecciona aleatoriamente una palabra correspondiente a la distribución de todo el vocabulario $V$

De esta forma, cada documento $d$ exhibe ciertos tópicos $t$ en diferente proporción (paso 1.) cada palabra $w$ es extraída de uno de los tópicos (paso 2.ii), donde el tópico selecionado es elegido de la distribución de tópicos de ese documento $d$ particular (paso 2.i).

Así, el objetivo del modelado de tópicos es descubrir de forma automática los temas a los que alude un determinado conjunto de documentos. Ahora bien, como puede intuirse esa estructura de tópicos puede ser pensada como un set de variables latentes a la TFM. Lo único observado es el conjunto de documentos (preprocesado como una TFM). La estructura de tópicos (es decir, la composición de tópicos por documento y la asignación de palabras a un documento) puede ser considerada como un conjunto de variables no observadas (justamente es lo que se trata de estimar). Formalizando el razonamiento anterior, es posible ver que se trata de una probabilidad conjunta:

$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) = \prod_{i=1}^K p(\beta_{i}) \prod_{d=1}^D p(\theta_{d}) (\prod_{n=1}^N p(z_{d,n}|\theta_{d}) p(w_{d,n}|\theta_{d}) p(w_{d,n}|\beta_{1:K}, z_{d,n}))$$

Así, los tópicos están indexados en $\beta_{1:K}$, dónde cada $\beta_{k}$ es una distribución de probabilidad sobre el vocabulario. La proporción de tópicos para el d-ésimo documento están indexadas por $\theta_{d}$, dónde $\theta_{d,K}$ es la proporción del tópico d en el documento $k$. La asignación de tópicos para el documento $d$, está dada por $z_{d}$, donde $z_{d,n}$ es la asignación de tópico para la n-ésima palabra en el documento $d$. Finalmente, las palabras observadas para el documento $d$, son $w_{d}$, donde $w_{d,n}$ es la n-ésima palabra en el documento $d$, que es un elemento del vocabulario fijo.

Puede verse que existen ciertas dependencias en el modelo: por ejemplo, la asignación de tópicos $z_{d,n}$ depende de la proporción de tópicos por documento $\theta_{D}$. A su vez, la palabra observada $w_{d,n}$ depende de la asignación de tópicos $z_{d,n}$ y todas dependen de los tópicos $\beta_{k}$.

Entonces, el problema es poder estimar la estructura de tópicos a partir de los documentos observados. De esta forma, es posible formular el problema a partir del llamado “posterior”:

$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D} | w_{1:D}) = \frac{p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D})}{p(w_{1:D})}
$$

El numerador es la distribución conjunta de todas las variables aleatorias del modelo y puede ser calcuado de forma simple. El problema es el denominador: la probabilidad marginal de las observaciones, es decir, la probabilidad de observar el corpus dado bajo cualquier modelo de tópicos. En teoría debería poder ser calculado a partir de la agregación de todas las distrbuciones de tópicos para cada una de las posibles estructuras de tópicos. El problema es que esas estructuras crecen de forma muy rápida y al igual que en muchos problemas dentro del marco bayesiano, es necesario recurrir a aproximaciones numéricas^[No es el objetivo de este trabajo desarrollar los métodos de inferencia y aproximación. En general, se basan en métodos de inferencia variacional o en métodos basados en Markov Chain Montecarlo. Para un mayor desarrollo puede verse [@asuncion].].

Ahora bien, el método utilizado tiene algunas supuestos que, si bien pueden deducirse de lo expuesto más arriba, será útil repasarlos:

1. Cada documento d se compone de varios tópicos
2. Un tópico, a su vez, se compone de palabras; más precisamente, un tópico es una distribución de probabilidad sobre la totalidad de palabras del vocabulario $V$
3. Los tópicos “preexisten” a los documentos
4. Dado que se basa en el modelo _BoW_, para la construcción de tópicos se asume que las palabras no tienen orden
5. A su vez, el orden de los documentos no es relevante. Se asume que existe una cantidad fija de tópicos (y que es un hiperparámetro del modelo). Esto puede ser un problema al analizar corpus con documetnos de épocas muy diferentes^[[@blei1] expone varios métodos para flexibilizar este supuesto. Particularmente, los llamados _dynamic topic modelling_ son una técnica posible.].


## 5. Resultados

Utilizando LDA se buscó detectar los tópicos más relevantes en el dataset de letras de tango.

Ahora bien, como se desprende del apartado anterior, uno de los problemas principales es determinar la cantidad de tópicos que se busca detectar. Este problema es análogo al problema de determinación de la cantidad de clusters al aplicar algoritmos de segmentación tales como _K-means_. Es más, en la etapa de preprocesamiento, también se tomaron una serie de decisiones: el tipo de TFM a utilizar y el método de normalización, etc. Es por ello, que sería posible considerar cada una de estas decisiones como un hiperparámetro a evaluar y testear todas las combinaciones posibles de estos hiperparámetros, junto con el parámetro obligatorio referido a la cantidad de tópicos a detectar en el corpus. 

Existen diversas métricas que permiten cuantificar qué tan “bueno” es el número de tópicos definido en términos cuantitativos (log-likelihood, perplexity, etc.). Ahora bien, en general el uso de estas métricas conduce a modelos que logran buena performance estadística pero no necesariamente generan tópicos que sean interpretables, es decir, que tengan algún sentido en términos semánticos. Más bien, tiende a suceder lo contrario.

En términos generales, un número de tópicos grande tiende a arrojar mejores métricas y tiende a permitir una alta resolución de la estructura latente del corpus. Ahora bien, se ha observado que a medida que medida que el número de tópicos se incrementa, la calidad de los tópicos (en términos de interpretabilidad) tiende a decrecer [@mimno, @chang].

De esta forma, al igual que en muchos otros problemas, complejidad del modelo e interpretabilidad tienden a ir en direcciones contrarias^[Existe otro conjunto de métricas -coherece, topic itrusion, etc..- que se centran en la interpretabilidad [@mimno].]. 

En el presente ejercicio se intentó buscar $k$ que permitiera identificar tópicos interpretables. En el anexo se presentan algunas de los principales términos para diferentes $k$.

Un primer rasgo que puede observarse es que, independientemente de las inicializaciones, existen algunos tópicos que se mantienen:

* Sentimientos y emociones con carácter positivo o negativo
* Imágenes de la noche, oscuridad y sombras asociadas a despedidas
* Imágenes que vinculan al tango y al barrio, arrabal
* Tópico sobre el tango, específicamente

Hemos llamado "misceláneos" a aquellos tópicos que presentan una distribución de palabras que no resultan interpretables. 

De esta forma, seleccionamos el modelo que muestra 12 topicos. Observemos las 30 primeras palabras de cada uno de los tópicos.

**Gráfico 1. Compisición de las 30 palabras de cada tópico ($k=12$)**

```{r echo=FALSE, fig.height=15, fig.width=15, message=FALSE, warning=FALSE}
library(ggwordcloud)
topic_names_12_06<-c('01 Imagenes climaticas', 
               '02 Ciudad, imagenes urbanas', 
               '03 Misc',
               '04 Emociones positivas', 
               '05 Campo y gauchesca',
               '06 Tango y arrabal',
               '07 Tiempo, recuerdos', 
               '08 Misc',
               '09 Emociones negativas',
               '10 Candombe',
               '11 Misc y familia',
               '12 Misc y lunfardo')


topic_terms<-list()
for (t in 1:12){
        topic_terms[[t]] <- get_topic_terms(lda=lda, topic_n=t, n_words = 30)
}

topic_terms <- do.call(rbind, topic_terms)

names(topic_names_12_06)<-1:12

topic_terms %>%
        ggplot(aes(label=term, size=prob, color=prob)) +
        geom_text_wordcloud(rm_outside = TRUE) +
        scale_size_area(max_size=25) +
        scale_fill_viridis_c() +
        facet_wrap(~topic, labeller= as_labeller(topic_names_12_06)) + 
        theme_minimal()
```


Puede verse que el primer tópico detectado tiene palabras como "noche", "luna", "cielo", "sombras", "viento". Es decir, nos habla de imágenes naturales o climáticas.

A su vez, el segundo tópico capta el tema de la ciudad y de las imágenes urbanas: menciona términos como "buenos", "aires", "ciudad", "calles". A su vez, el tópico 6 habla del arrabal, pero sobre todo del tango mismo ("tango", "barrio", "arrabal", "canción", "milonga", "bandonéon"). El tópico 7 ("pasado", "recuerdo", "tiempo") menciona palabras vinculadas al paso del tiempo y a la memoria.

Los tópicos 4 y 9 contienen palabras vinculadas a las emociones. El 4 ("ilusión", "pasión", "corazón", "amor") con una connotación positiva y el 9 ("amor", "dolor", "pena", "triste"), negativa.

El tópico 5 y el 10 logran evidenciar tópicos de carácter "étnico", por decirlo de alguna manera: el 5 con palabras como "china", "gaucho", "tierra", "sangre" capta la cuestión de la gauchesca. El tópico 10, en cambio, ("carnaval", "negro", "morena", "candombe") habla sobre el candombe y la cuestión de color.

Por último, restan cuatro tópicos. Dos de ellos (3 y 8) tienen un carácter residual. Resultan difíciles de interpretar. No obstante, el 11 y el 12, si bien contienen muchos términos que son poco interpetables, puede verse que el 11 ("vieja, "domingo", "niños") contiene palabras vinculadas a la vida familiar y el 12, términos en lunfardo ("bulín, "pinta", "che", "pibe). El tópico 12, también parece tener como sus dos palabras más importantes "vos" y "sos", con lo cual, parece estar captando el hecho de que se habla de forma directa a un interlocutor.

**Tabla 3. Identificación de los tópicos hallados**

| Tópico | Etiqueta |
|--------|----------|
| 01 | Imagenes climaticas      |
| 02 | Ciudad, imagenes urbanas |
| 04 | Emociones positivas      |
| 05 | Campo y gauchesca        |
| 06 | Tango y arrabal          |
| 07 | Tiempo, recuerdos        |
| 09 | Emociones negativas      |
| 10 | Candombe                 |
| 11 | Misc y familia           |
| 12 | Misc y lunfardo          |


Ahora bien, una vez detectados los tópicos podemos estimar para cada documento $d$ del corpus $C$ que proporción presenta de cada uno de los tópicos. A continuación se exponen la composición de tópicos de seis tangos de tres décadas diferentes:

**Gráfico 2. Composición de tópicos según tangos, 1900-2010**

```{r echo=FALSE, fig.height=6, fig.width=8, message=FALSE, warning=FALSE}
doc_topics<-get_topic_per_doc(lda)
colnames(doc_topics)<-topic_names_12_06

tangos<-c('barrio reo', 'mi buenos aires querido',
          'garua',
          'chorra', 'malena')

tangos_test <- doc_topics[match(tangos, df$titulo),]

colnames(tangos_test)<-topic_names_12_06

tangos_test %>% 
        as_tibble() %>%
        mutate(tango=tangos) %>%
        select(tango, everything()) %>%
        gather(topic, value,`01 Imagenes climaticas`:`12 Misc y lunfardo`) %>%
        arrange(tango, topic) %>%
        ggplot(aes(x=tango, y=value, fill=topic)) + 
                geom_bar(stat = "identity") + ylab("value") +
        geom_text(aes(label = round(value,2)), 
                  position=position_stack(vjust = 0.5),
                  check_overlap = TRUE) 
#+
#        labs(title='Composición de tópicos según tangos, 1900-2010',
#             subtitle='Media de la composición de las letras de tango')
```

Así, un tango como "Barrio Reo", que habla de los recuerdos gratos recuerdos del cantor al retornar a su barrio y de la tristeza que le produce el encuentro con su deterioro ("Hoy te encuentro envejecido"), muestra valores altos en los tópicos _emociones positivas_, _emociones negativas_ y en el que habla sobre el _tango y arrabal_.

Al mismo tiempo, "Chorra", el tópico predominante es el 12. En efecto, el tango está dedicado y dirigido a una persona (la "chorra" en cuestión) y, al mismo tiempo, utiliza una buena cantidad de términos del lunfardo ("afanaste", "chorra", "cachaban", "gil").

El tango "Garúa" habla sobre una caminata del narrador bajo la llovizna y pinta un cuadro lúgubre y oscuro ("sobre la calle la hilera de focos lustra el asfalto con luz mortecina") mientras el caminante recuerda a la mujer que, presumiblemente se fue. Es por eso que las _emociones negativas_ y las _imágenes climáticas_ aparecen con fuerza en este tango.

"Malena" nos habla (en tercera persona) de una cantante de tangos que pasó por desamores, que parece tener la bebida fácil y que "canta el tango como ninguna". Esto se corresponde con la composición de tópicos detectada: _emociones negativas_, _tango y arrabal_ e _imágenes climáticas_ ("tono oscuro", "el frío del último encuentro", "tus manos son palomas que sienten frío").

Por úlimo, "Mi Buenos Aires Querido", nos habla de la ciudad, la nostalgia del narrador y la esperanza del retorno. Esto se corresponde con los tópicos detectados: _ciudad_, _emociones negativas_, _emociones positivas_ y _tiempo y recuerdos_.

Al mismo tiempo, podemos analizar la evolución de cada uno de los tópicos a lo largo de las diferentes décadas.

**Gráfico 4. Evolución de los tópicos, 1900-2010 (suavizado GAM)**

```{r echo=FALSE, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}

df %>%
        select(decada, ano) %>%
        bind_cols(., as.data.frame(doc_topics)) %>%
        gather(topic, value, `01 Imagenes climaticas`:`12 Misc y lunfardo`) %>%
        ggplot() +
                #geom_point(aes(x=ano, y=value, color=topic)) +
                geom_smooth(aes(x=ano, y=value, color=topic)) +
                facet_wrap(~topic) +
                theme_minimal() +
                theme(legend.position = 'none')
```


A partir de la evolución temporal puede verse cómo efectivamente van modificándose los temas del tango. En primer lugar, las imágenes naturales y climáticas ganan predominio de forma casi sostenida a lo largo del tiempo. En mucha menor medida, el tópico vinculado a la ciudad parece ganar cierta importancia a partir de la década del '30. También resultan interesantes las oscilaciones que parece presentar el tema del tango y el arrabal. Parece caer levemente hacia la década del '20 y vuelve a incrementarse hacia los años '50, mostrando otro valle hacia los años '70.

Pero quizás uno de los cambios más importantes es el que se observa en los tópicos 4 y 9. En efecto, puede verse que la participación de las emociones positivas es relativamente constante a lo largo del tiempo. En cambio, son las emociones negativas las que muestran diferencias fundamentales a lo largo del tiempo: muestran una tendencia al crecimiento hasta la década del '40-'50 y luego tienden a bajar de forma más suave.

Este punto es interesante porque toca algunas de las discusiones dentro del campo de los estudios del tango. Así, por ejemplo Borges [-@borges], planteaba que 

> “el tango, como hemos visto, empezó, surge de la milonga, y es al principio un baile valeroso y feliz. Y luego, el tango va languideciendo y entristeciéndose..."

De esta forma, puede verse que esta hipótesis parece ser corroborada por la información construida^[En el mismo texto, Borges [-@borges] discute sobre las causas de dicho cambio **DESARROLLAR**].

Qué vinculación existe entre éstos canbios en los temas del tango u los procesos de desarrollo del capitalismo en Argentina y a los procesos de migración rural-urbana es una pregunta de interés para futuros pasos.

Por úlitmo, y para mostrar una última posible aplicación, podemos calcular la composición promedio de los diferentes tópicos en cada autor de tango. A continuación, desplegamos la composición de los cinco autores con mayor cantidad de letras en el dataset.


**Gráfico 5. Composición de tópicos según autores, 1900-2010. Media de la composición de las letras de tango**

```{r echo=FALSE, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
autores <- df %>% 
        group_by(compositor) %>%
        summarise(n=n()) %>% 
        arrange(desc(n)) %>%
        top_n(5) %>%
        select(compositor)


aggregate(doc_topics, 
          by = list(author = df$compositor), 
          mean) %>%
        filter(author %in% autores$compositor) %>%
        gather(topic, value,`01 Imagenes climaticas`:`12 Misc y lunfardo`) %>%
        ggplot(aes(x=author, y=value, fill=topic)) + 
                geom_bar(stat = "identity") + ylab("value") +
                geom_text(aes(label = round(value,2)), 
                  position=position_stack(vjust = 0.5),
                  check_overlap = TRUE) +
        coord_flip() +
        theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Así, por ejemplo, los temas predominantes de Cadícamo parecen ser el lunfardo y las emociones negativas. Esto contrasta con Homero Manzi, quien parece utilizar en mayor medida las imágenes climaticas (y también las emoicones negativas). 

De esta forma, podría pensarse en construir una matriz de distancias para cada autor en función de la composición promedio de sus tópcios, con el objetivo de encontrar autores que utilizan temas similares ^[De forma análoga podría construirse una matriz a nivel de tango y buscar los tangos que hablan de tópicos similares.].


**Gráfico 6. Distancias en la composición de tópicos según autores, 1900-2010. Media de la composición de las letras de tango**

```{r echo=FALSE, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
autores <- df %>% 
        group_by(compositor) %>%
        summarise(n=n()) %>% 
        
        arrange(desc(n)) %>%
        top_n(20) %>%
        select(compositor) %>%
        pull()
       
aggregate(doc_topics, 
          by = list(author = df$compositor), 
          mean) %>%
        filter(author %in% autores) %>%
        dist(.) %>%
        usedist::dist_setNames(., autores) %>%
        broom::tidy(.) %>%
        ggplot(., aes(item1, item2, fill= distance)) + 
        geom_tile() +
        scale_fill_viridis_c() +
        theme(axis.text.x = element_text(angle=90))

```

De esta forma, puede verse por ejemplo, que Celedonio Flores y Homero Manzi parece ser de los que mayor similutdes tienen en relación a los tópicos que utilizan. Algo parecido pasa con Enrique Dizeo y José María Contursi, por un lado y con Ernesto Pierro por el otro.


## 6. Resultados y discusión

En el presente trabajo se buscó presentar una aproximación metodológica posible para el análisis automático de textos. A partir de la aplicación de una técnica de detección de tópicos (LDA) sobre un dataset de 5700 letras de tango. A su vez, se presentó un flujo de trabajo posible para dicho análisis y se discutieron algunas técnicas para el preprocesmaiento del texto.

De esta forma, fue posible estimar, mediante la técnica de topic modeling LDA, los principales temas del tango. Así, el uso de emociones positivas y negativas, imágenes de la ciudad, sobre el tango y el arrabal, sobre el campo y la gauchesca, sobre la temporalidad y la memoria, entre otros, aparecían como los más importantes. Al mismo tiempo, fue posible validar los tópicos seleccionando algunos tangos y analizando sus letras y su correspondencias con los tópicos estimados.

Quizás uno de las posibilidades analíticas más interesantes fue la de poder analizar la evolución temporal de los tópicos y, eventualmente, plantear hipótesis sobre la vinculación con procesos m

Finalmente, fue posible analizar las diferencias (distancias) entre los tópicos utilizados por diferentes autores.

Ahora bien, más allá de los resultados del ejercicio propuesto (que tienen como objetivo más mostrar un caso de uso de la herramienta que agotar las determinaciones del objeto en cuestión), el trabajo busca mostrar las potencialidades que este tipo de técnicas tienen para la investigación en ciencias sociales. Particularmente, la detección de tópicos mediante LDA ha sido utilizada en los últimos tiempos al análisis literario [@jockers], al estudio de comunicados políticos [@grimmer2], al estudio de medios [@wind, @blei2], al estudio de temas en leyes y proyectos [@blei3], por nombrar algunas aplicaciones relevantes.

En efecto, sus principales ventajas radican en su escalabilidad y replicabilidad: utilizando las técnicas de análisis cualitativo de textos "tradicionales", es posible lograr gran profundidad analítica pero sobre corpus más bien pequeños o medianos y escasamente replicables. Así, los trabajos mencionados tenían una escala más bien pequeña: alrededor de 30 letras de tango. La excepción es el trabajo de [@canton]. El proceso de detección de tópicos encarado en el presente trabajo procesó y analizó una base de datos de alrededor de 6.200 letras de tango.

A su vez, el uso de técnicas automáticas de NLP no implica un desplazamiento de los enfoques tradicionales. Un buen ejemplo es el trabajo de [@baumer], en el que se comparan los resultados obtenidos utilizando dos métodos de análisis sobre un mismo corpus de datos: generación de categorías utilizando la metodología de la _Grounded Theory_ y detección de tópicos utilizando LDA. Los resultados sugieren tanto una coherencia como una complementariedad entre los resultados de ambos métodos.


# Bibliografía

# Anexo - Tablas con los primeros 8 términos para estimación de tópicos con diferentes $k$.

**Tabla A.1 $k$=15** 
```{r echo=FALSE, fig.height=15, fig.width=15, message=FALSE, warning=FALSE}

knitr::kable(terms(lda_list[[2]],8) %>% t() %>% as_tibble() %>% mutate(topic=paste('Topic',1:15)) %>% select(topic, everything()))
```

**Tabla A.2 $k$=13**
```{r echo=FALSE, fig.height=15, fig.width=15, message=FALSE, warning=FALSE}

knitr::kable(terms(lda_list[[6]],8) %>% t() %>% as_tibble() %>% mutate(topic=paste('Topic',1:13)) %>% select(topic, everything()))
```

**Tabla A.1 $k$=10**
```{r echo=FALSE, fig.height=15, fig.width=15, message=FALSE, warning=FALSE}

knitr::kable(terms(lda_list[[12]],8) %>% t() %>% as_tibble() %>% mutate(topic=paste('Topic',1:10)) %>% select(topic, everything()))
```

**Tabla A.1 $k$=5**
```{r echo=FALSE, fig.height=15, fig.width=15, message=FALSE, warning=FALSE}

knitr::kable(terms(lda_list[[22]],8) %>% t() %>% as_tibble() %>% mutate(topic=paste('Topic',1:5)) %>% select(topic, everything()))
```

